% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
\usepackage{booktabs}


\begin{document}
	
	% Copyright
	\setcopyright{acmcopyright}
	%\setcopyright{acmlicensed}
	%\setcopyright{rightsretained}
	%\setcopyright{usgov}
	%\setcopyright{usgovmixed}
	%\setcopyright{cagov}
	%\setcopyright{cagovmixed}
	
	
	% DOI
	\doi{n/a}
	
	% ISBN
	\isbn{n/a}
	
	%Conference
	%\conferenceinfo{PLDI '13}{June 16--19, 2013, Seattle, WA, USA}
	
	%\acmPrice{\$15.00}
	
	%
	% --- Author Metadata here ---
	%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
	%\CopyrightYear{2015} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
	%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
	% --- End of Author Metadata ---
	
	\title{Analysis of Classification Techniques for Prediction of Tuberculosis Defaulters}
	%
	% You need the command \numberofauthors to handle the 'placement
	% and alignment' of the authors beneath the title.
	%
	% For aesthetic reasons, we recommend 'three authors at a time'
	% i.e. three 'name/affiliation blocks' be placed beneath the title.
	%
	% NOTE: You are NOT restricted in how many 'rows' of
	% "name/affiliations" may appear. We just ask that you restrict
	% the number of 'columns' to three.
	%
	% Because of the available 'opening page real-estate'
	% we ask you to refrain from putting more than six authors
	% (two rows with three columns) beneath the article title.
	% More than six makes the first-page appear very cluttered indeed.
	%
	% Use the \alignauthor commands to handle the names
	% and affiliations for an 'aesthetic maximum' of six authors.
	% Add names, affiliations, addresses for
	% the seventh etc. author(s) as the argument for the
	% \additionalauthors command.
	% These 'additional authors' will be output/set for you
	% without further effort on your part as the last section in
	% the body of your article BEFORE References or any Appendices.
	
	%\numberofauthors{8} %  in this sample file, there are a *total*
	% of EIGHT authors. SIX appear on the 'first-page' (for formatting
	% reasons) and the remaining two appear in the \additionalauthors section.
	%
	\author{
		% You can go ahead and credit any number of authors here,
		% e.g. one 'row of three' or two rows (consisting of one row of three
		% and a second row of one, two or three).
		%
		% The command \alignauthor (no curly braces needed) should
		% precede each author name, affiliation/snail-mail address and
		% e-mail address. Additionally, tag each line of
		% affiliation/address with \affaddr, and tag the
		% e-mail address with \email.
		%
		% 1st. author
		\alignauthor
		Brian Mc George\\
		\affaddr{University of Cape Town}\\
		\affaddr{Cape Town, South Africa}\\
		\email{mcgbri004@myuct.ac.za}
	}
	
	\maketitle
	\begin{abstract}
	\end{abstract}
	
	%
	%  Use this command to print the description
	%
	\printccsdesc
	
	% We no longer use \terms command
	%\terms{Theory}
	
	%\keywords{ACM proceedings; \LaTeX; text tagging}
	
	\section{Introduction}
	In 2013 over 210\hspace*{1mm}000 patients defaulted from Tuberculosis (TB) treatment worldwide \cite{world2015TB}. The rate of default in the Americas is the highest at 8\% with Africa at 5\% \cite{world2015TB}. The consequences of defaulting TB treatment include: increased drug resistance, increased health system costs \cite{Lackey:10356751520150601, muture:6660173120110101}, higher risk of mortality, continued risk of transmitting the disease to others \cite{Lackey:10356751520150601} and increased rate of recurrent disease \cite{Jha:10.1371/journal.pone.0008873}. The spread of TB can be reduced if the individuals who have a high risk of defaulting can be predicted. This will also reduce health system costs.
	
	In this paper we examine a wide range of classification techniques at their ability to predict patients as defaulters. We explore a number of techniques and strategies to improve the overall classification result. Most notably we look at how well these classifiers work out-of-the-box compared to when they are hand optimised or tuned by searching a grid of parameters. We examine different data balancing techniques which either over-sample the minority class or under-sample the majority class. Furthermore, to see how each classifier responds to having fewer features available, we apply a number of feature selection strategies on each classification technique.
	
	To facilitate the aforementioned experiments, we developed a testing system which allows new classification techniques and data sets to be supported quickly and easily. The testing system is designed to allow near-exact reproducibility of results.
	
	The field of credit scoring in the financial space aims to determine if a financial institution should provide credit to an individual. This area has been well researched. In this paper we aim to determine if our results generalise across the different fields. There are notable similarities in these problems which could make them comparable. Both problems typically have a labelled dataset consisting of both nominal and numerical data as well as the occurrence of missing data [citation needed]. However, TB datasets are more prone to missing data as well as inaccuracies due to the nature of the data collection [citation needed]. The selected classification techniques are evaluated against real-world treatment default datasets and financial datasets. The paper will evaluate how the techniques differ across the datasets. If the relative results are similar then future credit scoring research could be applicable to treatment default prediction too.
	
	\section{Related work}
	\subsection{Definition of a defaulter}
	The definition of a defaulter depends on its context. TB literature typically uses the World Health Organisation (WHO) definition that a defaulter is a person whose treatment has been disrupted for two or more consecutive months \cite{chan:2003prevalence, cherkaoui:19326203, Jha:10.1371/journal.pone.0008873,jittimanee:10.1111/j.1440-172X.2007.00650.x,muture:6660173120110101, world2015TB}.
	
	\subsection{Determining predictors of TB default}
	There have been many studies which focus on determining the factors associated with TB default but few have used machine learning techniques to predict treatment defaulters. Table \ref{table:predictors_of_tb_default} contains an overview of a selection of publications on determining the factors associated with TB default. The majority of techniques use a form of logistic regression ti determine the association. 
	
	The datasets used by the publications contain different features. Age and gender are common throughout the datasets. History of past default is available for all datasets except for Shargie \textit{et al.} \cite{Shargie:10.1371/journal.pmed.0040037}. Lackey \textit{et al.} \cite{Lackey:10356751520150601} only picked individuals who did not have a history of past default. Jittimanee \cite{jittimanee:10.1111/j.1440-172X.2007.00650.x} [28] was the only publication with the feature that did not find it to be significant to the 95\% confidence level. However, it did have an odds ratio of 2.19 and a p-value of 0.12. It can therefore be deduced that a history of past default has a strong correlation to default. Two out of three publications with the alcohol abuse feature available, found it to be significant. Three of the four publications with side effects, as a feature found it was significant. Shargie \textit{et al.} \cite{Shargie:10.1371/journal.pmed.0040037} and Jittimanee \textit{et al.} \cite{jittimanee:10.1111/j.1440-172X.2007.00650.x} measured distance and time to treatment site respectively. It can be reasoned that the aforementioned feature's significance will generalise to other datasets since they were found to be significant in the majority of the publications. Other significant features such as illegal drug use, use of herbal medication, daily jobs, history of lung cancer and history of liver disease only appeared once in the datasets. It cannot be discerned if the significance is generalisable or specific to the dataset. The identification of the same features as significant is fairly consistent for the publications that have those features in their dataset. 
	
	\begin{table*}
		\small
		\centering
		\caption{Overview of publications on predictors of TB treatment defaulters}
		\label{table:predictors_of_tb_default}
		\makebox[\linewidth]{
			\begin{tabular}{l|c|c|c} \hline
				Publication&Sample Size&Key factors identified\textsuperscript{*}&Evaluation\rule{0pt}{3.5mm}\rule[-1mm]{0pt}{0pt}\\ \hline
				\parbox[t]{1.5cm}{Chan-Yeung \textit{et al.} \cite{chan:2003prevalence}}
				&\parbox[t]{2.6cm}{1768 non-defaulters\\442 defaulters.}
				&\parbox[t]{7cm}{History of default, history of lung cancer, liver disease and male patients}
				&\parbox[t]{8cm}{Multiple logistic regression is used to determine what factors are associated with default.}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
				\parbox[t]{1.5cm}{Jha \textit{et al.} \cite{Jha:10.1371/journal.pone.0008873}}
				&\parbox[t]{2.6cm}{1189 non-defaulters\\1141 defaulters.}
				&\parbox[t]{7cm}{Male patients, patients directly-observed treatment at public facilities, previous treatment outside India's Revised National Tuberculosis Control Programme and history of previous default\vspace{1mm}}
				&\parbox[t]{8cm}{The chi-square test or Fisher's exact test (if there were less than 10 observations) was used to test the differences between defaulters and non-defaulters. Bivariate analysis was calculated on the features. Multivariate logistic regression using pre-selected features based on previous studies.}\rule{0pt}{3.5mm}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
				\parbox[t]{1.5cm}{Jittimanee \textit{et al.} \cite{jittimanee:10.1111/j.1440-172X.2007.00650.x}}
				&\parbox[t]{2.6cm}{106 non-defaulters\\54 defaulters.}
				&\parbox[t]{7cm}{Jobs where one is only paid if one is at work that day, severe medication side-effects and time to travel to clinic.}
				&\parbox[t]{8cm}{Patients were interviewed and completed a questionnaire to obtain the information. Hierarchical logistic regression  was carried out to assess the variable's relation to default.}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
				\parbox[t]{1.5cm}{Lackey \textit{et al.} \cite{Lackey:10356751520150601}}
				&\parbox[t]{2.6cm}{1106 non-defaulters\\127 defaulters.}
				&\parbox[t]{7cm}{Has used illegal drugs, has multidrug-resistant TB, has not been tested for HIV, drinks alcohol at least once a week, underweight or has not completed secondary education.}
				&\parbox[t]{8cm}{{Patients were interviewed to obtain the information. Bivariate analysis using Chi-square tests and odds ratios with 95\% confidence intervals. Multivariate logistic regression is used with a backward fitting algorithm to determine if a variable is associated with default.}}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline		
				\parbox[t]{1.5cm}{Muture \textit{et al.} \cite{muture:6660173120110101}}
				&\parbox[t]{2.6cm}{5659 non-defaulters\\945 defaulters.}
				&\parbox[t]{7cm}{Inadequate knowledge on TB, herbal medication use, low income, alcohol abuse, previous default, suffering from HIV and male patients were determined through analysis to be associated with default.}
				&\parbox[t]{8cm}{Two-tailed $\chi^2$ tests and Fisher exact tests (if a cell has less than 5 values) to assess categorical information. Odds ratio tests were used to measure association between features with a 95\% confidence interval.}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
				\parbox[t]{1.5cm}{Shargie \textit{et al.} \cite{Shargie:10.1371/journal.pmed.0040037}}
				&\parbox[t]{2.6cm}{310 non-defaulters\\81 defaulters.}
				&\parbox[t]{7cm}{Distance from home to treatment site, age greater than 25 and the need to use public transport to get to treatment site.}
				&\parbox[t]{8cm}{Continuous features were analysed using sample t-test $\chi^2$ tests and hazard ratios with 95\% confidence intervals. The effect of each factor was assessed using Cox regression model with a backwards fitting algorithm.}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
				\noalign{\vskip 4pt}                     
				\multicolumn{4}{l}{\textsuperscript{*}\footnotesize{\parbox[t]{\linewidth}{Ordered in descending order of significance  based on odds ratio and hazard ratio at a confidence interval of at least 95\%. }}}\\
			\end{tabular}
		}
	\end{table*} 
	
	\subsection{Predicting defaulters in financial institutions}
	\begin{table}
		\small
		\caption{German data set\textsuperscript{\textparagraph}}
		\label{table:german_dataset}	
		\begin{tabular}{c|c|c|c} \hline		
			Publication&Accuracy&\parbox[t]{1.2cm}{\centering Type I\\Error}&\parbox[t]{1.2cm}{\centering Type II\\Error}\rule{0pt}{3mm}\rule[-0mm]{0pt}{0pt}\\ \hline
			\parbox[t]{2.3cm}{Huang \textit{et al.} \cite{Huang2004543}}
			&\parbox[t]{2.3cm}{\centering \textbf{79.87\%}\textsuperscript{\textdagger}}
			&\parbox[t]{1.1cm}{\centering n/a}
			&\parbox[t]{1.2cm}{\centering n/a}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
			\parbox[t]{2.3cm}{Li \textit{et al.} \cite{Li2006772}}
			&\parbox[t]{2.3cm}{\centering \textbf{84.83\%}}
			&\parbox[t]{1.1cm}{\centering 10-20\%\textsuperscript{*}}
			&\parbox[t]{1.2cm}{\centering 10-20\%\textsuperscript{*}}\\ \hline
			\parbox[t]{2.3cm}{Luo \textit{et al.} \cite{Luo20097562}}
			&\parbox[t]{2.3cm}{\centering 77.06\% (MySVM), 82.41\% (SVM-GA)\textsuperscript{\textdagger}}
			&\parbox[t]{1.1cm}{\centering n/a}
			&\parbox[t]{1.2cm}{\centering n/a}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
			\parbox[t]{2.3cm}{Huang \textit{et al.} \cite{Huang2007847}}
			&\parbox[t]{2.3cm}{\centering 82.41\% (SVM-GA)\textsuperscript{\textdagger}}
			&\parbox[t]{1.1cm}{\centering n/a}
			&\parbox[t]{1.2cm}{\centering n/a}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
			\parbox[t]{2.3cm}{Danenas \textit{et al.} \cite{Danenas20153194}}
			&\parbox[t]{2.3cm}{\centering \textbf{94.41\%} (Linear SVM), \textbf{92.37\%} (PSO-LinSVM)\textsuperscript{\textdagger}}
			&\parbox[t]{1.1cm}{\centering n/a\textsuperscript{\textdaggerdbl}}
			&\parbox[t]{1.2cm}{\centering n/a\textsuperscript{\textdaggerdbl}}\rule{0pt}{3.5mm}\rule[-8mm]{0pt}{0pt}		
		\end{tabular}		
	\end{table}
	
	\begin{table}
		\small
		\caption{Australian data set\textsuperscript{\textparagraph}}
		\label{table:australian_dataset}	
		\begin{tabular}{c|c|c|c} \hline		
			Publication&Accuracy&\parbox[t]{1.2cm}{\centering Type I\\Error}&\parbox[t]{1.2cm}{\centering Type II\\Error}\rule{0pt}{3mm}\rule[-0mm]{0pt}{0pt}\\ \hline
			\parbox[t]{2.3cm}{Huang \textit{et al.} \cite{Huang2004543}}
			&\parbox[t]{2.3cm}{\centering \textbf{79.87\%}\textsuperscript{\textdagger}}
			&\parbox[t]{1.1cm}{\centering n/a}
			&\parbox[t]{1.2cm}{\centering n/a}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
			\parbox[t]{2.3cm}{Li \textit{et al.} \cite{Li2006772}}
			&\parbox[t]{2.3cm}{\centering \textbf{84.83\%}}
			&\parbox[t]{1.1cm}{\centering 10-20\%\textsuperscript{*}}
			&\parbox[t]{1.2cm}{\centering 10-20\%\textsuperscript{*}}\\ \hline
			\parbox[t]{2.3cm}{Luo \textit{et al.} \cite{Luo20097562}}
			&\parbox[t]{2.3cm}{\centering 77.06\% (MySVM), 82.41\% (SVM-GA)\textsuperscript{\textdagger}}
			&\parbox[t]{1.1cm}{\centering n/a}
			&\parbox[t]{1.2cm}{\centering n/a}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
			\parbox[t]{2.3cm}{Huang \textit{et al.} \cite{Huang2007847}}
			&\parbox[t]{2.3cm}{\centering 82.41\% (SVM-GA)\textsuperscript{\textdagger}}
			&\parbox[t]{1.1cm}{\centering n/a}
			&\parbox[t]{1.2cm}{\centering n/a}\rule{0pt}{3.5mm}\rule[-0mm]{0pt}{0pt}\\ \hline
			\parbox[t]{2.3cm}{Danenas \textit{et al.} \cite{Danenas20153194}}
			&\parbox[t]{2.3cm}{\centering \textbf{94.41\%} (Linear SVM), \textbf{92.37\%} (PSO-LinSVM)\textsuperscript{\textdagger}}
			&\parbox[t]{1.1cm}{\centering n/a\textsuperscript{\textdaggerdbl}}
			&\parbox[t]{1.2cm}{\centering n/a\textsuperscript{\textdaggerdbl}}\rule{0pt}{3.5mm}\rule[-8mm]{0pt}{0pt}		
		\end{tabular}		
	\end{table}

	
	\section{Background}
	This section aims to provide an overview of all the techniques used in this paper.
	\subsection{Classifiers}
	This section examines the different classification techniques and outlines how each technique produces its output.
	\subsubsection{Support Vector Machines}
	Support vector machine (SVM) is a machine learning technique that can be used to produce regression or classification functions from a set of training data \cite{Luo20097562}. SVM works by mapping the input vectors into a high-dimensional feature space with the use of a kernel function \cite{Danenas20153194}. The kernel function selected determines the if the mapping is linear or non-linear \cite{Luo20097562}. Linear, polynomial and radial basis function (RBF) are common kernel functions \cite{hsu2003practical}. The polynomial and RBF kernel performs a non-linear mapping into the high-dimensional space \cite{hsu2003practical}. This feature space is then searched to acquire an optimal hyperplane that separates the space with the maximum distance between the two classes \cite{Danenas20153194}. Hsu \textit{et al.} \cite{hsu2003practical} recommends the RBF kernel as a reasonable first choice but notes that it is not suitable when there are a large number of features. The linear kernel is recommend when there are a large number of features.
	
	\subsubsection{Artificial Neural Network}
	An artificial neural network (ANN) is based on the functionality of the human brain \cite{Wang2003}. Neurons in the brain are interconnected and process information in parallel \cite{Wang2003}. A typical ANN is comprised of an input layer, $k$ number of hidden layers and an output layer. The neurons in each layer are connected to the neurons in the next layer. A numeric weight is defined between each pair of connected neurons. An activation function defines if a neuron will fire \cite{Wang2003}. The activation function bounds the value of a neuron to a specific range to limit the effect of divergent neurons \cite{Wang2003}. By using an activation function, a non-linear combination of weights can be generated \cite{Wang2003}. It has been proven that an ANN is able to approximate any continuous function if has at least one hidden node and an activation function that is both bounded to some range and non-constant \cite{Hornik1991251}.
	
	Extreme learning machines (ELM) is an alternative approach to the conventional back-propagated ANN. Huang \textit{el al.} \cite{Huang2006489} proved that the input and hidden layer weights can be randomly assigned if the activation function in the hidden layer is infinitely differentiable. By randomly assigning these weights, the weights for the output nodes can be determined analytically \cite{Huang2006489}. This allows ELMs to be trained orders of magnitude faster than a back-propagated ANN \cite{Huang2006489} \cite{6035797}. Various studies have shown ELMs to provide better results than SVMs and ANNs on classification and regression tasks \cite{Huang2006489} \cite{6035797}.
	
	\subsubsection{Logistic Regression}
	Logistic regression is a technique that models the chance of an outcome based on the input features \cite{doi:10.11613/BM.2014.003}. Since chance is a ratio, the logarithm of the chance is what is modelled instead \cite{doi:10.11613/BM.2014.003}:
	log$(\frac{\mathcal{P}}{1 - \mathcal{P}}) = \beta_0 + \beta_1 x_1 + ... + \beta_m x_m$. $\mathcal{P}$ represents the probability of an event (likelihood to default for example). $\beta_0$ represents the value of the criterion when the predictor is equal to 0. $\beta_1, ..., \beta_m$ are the regression coefficients associated with the $x_1, ..., x_m$ input features. The probability of an event can then be calculated as $\mathcal{P}$ = $\frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_m x_m)}}$. A detailed overview of logistic regression can be found in \cite{Mood01022010} and \cite{doi:10.11613/BM.2014.003}.
	
	\subsubsection{$k$-Nearest Neighbours}
	The $k$-Nearest Neighbours ($k$NN) algorithm determines the output classification by examining the $k$ nearest training examples in the feature space \cite{6313426}. An input is classified by the majority vote of its neighbours.
	
	\subsubsection{Ensembles}
	An ensemble classifier is one which typically makes use of multiple classifiers and aggregate the results. A decision tree can be used for classification by branching on conjunctions of features and having the leaves represent the output class. A decision tree allows for easy interpretation of the generated model, however, typically provides relatively poor classification accuracy \cite{doi:10.1021/ci034160g}. 
	
	Random forest is a technique that fits a number of decision trees on \textit{random samples with replacement} of the dataset. For each tree, $n$ features are randomly selected and the tree is grown \cite{WIDM:WIDM1072}. Data that was not selected for training the tree is called out-of-bag data \cite{WIDM:WIDM1072}. It is used to test the error rate of the tree \cite{WIDM:WIDM1072, doi:10.1021/ci034160g}. The output classification is decided by the majority vote of each tree \cite{WIDM:WIDM1072}.
	
	Freund and Schapire's \cite{FREUND1997119} AdaBoost uses a weighted sum of multiple classifiers in order to determine the output. A classifier is constructed in an iterative fashion. At each iteration a pool of classifiers are considered and one classifier is added to the committee of classifiers. Input which is still misclassified is assigned a higher weight at each iteration \cite{Bergstra2006, rojas2009adaboost}. The aim being that the process will select a classifier which helps the still misclassified inputs. The chosen classifier is assigned a weight which determines its power in the overall classification \cite{Bergstra2006, rojas2009adaboost}. The sign of the function: $C(x) = \sum_{i=1}^{m} \alpha_i k_i$ is used to determine the final classification with $\alpha_i$ denoting the weight of each classifier $k_i$ \cite{Bergstra2006}. 
	
	\subsubsection{Naive Bayes}
	Naive Bayes makes an assumption that each input feature is independent of each other \cite{Lewis1998, rish2001empirical}. This allows multiple features be uncoupled from one another which simplifies the algorithm. Naive Bayes uses conditional probability to classify new inputs \cite{Lewis1998}. Using Bayes theorem, the following equation is derived: $p(C_i|\textbf{x}) = \frac{p(C_i) \times p(x_1|C_i) \times...\times p(x_n|C_i)}{p(\textbf{x})}$ with input $\textbf{x} = (x_1,...,x_n)$ and class label $C_i$ \cite{Lewis1998, rish2001empirical}. Since $p(\textit{x})$ is identical for each class, it is typically ignored \cite{rish2001empirical}. To classify an input, the probabilities for each class are calculated using the aforementioned equation and the output is the class with the highest probability \cite{Lewis1998}.
	
	\subsubsection{Clustering-launched classification}
	Clustering-launched classification (CLC) is a binary classifier. CLC first clusters the data into groups using diverse hierarchical k-means algorithm \cite{Luo20097562}. The clusters are divided into positive subsets and negative subsets \cite{Luo20097562}. Support vectors are then used to separate the positive and negative subsets for each cluster \cite{Luo20097562}. Redundant or repeated support vectors are removed thereafter \cite{Luo20097562}.
	
	\subsection{Data Balancers}
	\subsubsection{Over-sampling}
	Random over-sampling (RUS) is a technique that randomly replicates examples in the minority class \cite{Batista:2004:SBS:1007730.1007735}. However, this technique can increase the likelihood of over-fitting since exact copies are made from the minority class \cite{Batista:2004:SBS:1007730.1007735}.
	
	The Synthetic minority over-sampling technique (SMOTE) \cite{Chawla:2002:SSM:1622407.1622416} forms new minority samples by interpolating along the line segment on some or all of the $k$ nearest minority class neighbours of a minority example. This approach attempts to alleviate the over-fitting that can occur from using random over-sampling.
	
	Adaptive synthetic sampling (ADASYN) \cite{4633969} is a variation of SMOTE which uses a weighed distribution for different minority class examples according to their difficulty in learning. More synthetic data is generated for minority class examples that are more difficult to learn compared to those that are easier to learn.
	
	\subsubsection{Under-sampling}
	Random over-sampling (ROS) is a technique that randomly eliminates examples in the majority class \cite{Batista:2004:SBS:1007730.1007735}. However, this technique can remove potentially useful information from the training set \cite{Batista:2004:SBS:1007730.1007735}. 
	
	Condensed Nearest Neighbour (CNN) rule \cite{1056066} finds a consistent subset of examples. A subset $D$ of $E$ is consistent if using 1-nearest neighbour using $D$ correctly classifies $E$ \cite{Batista:2004:SBS:1007730.1007735}. The process draws one random majority class example and all minority class examples and puts them in $D$ \cite{Batista:2004:SBS:1007730.1007735}. Every misclassified example is then moved from $E$ to $D$ \cite{Batista:2004:SBS:1007730.1007735}. This process attempts to remove examples that are far away from the decision border and therefore seen as less relevant for learning \cite{Batista:2004:SBS:1007730.1007735}.
	
	The Tomek link (TL) algorithm \cite{4309452} examines two examples $\textbf{x}_i$ and $\textbf{x}_j$ belonging to different classes. A TL occurs if there is not an example $\textbf{x}_l$ such that $d(\textbf{x}_i, \textbf{x}_l) < d(\textbf{x}_i, \textbf{x}_j)$ or $d(\textbf{x}_j, \textbf{x}_l) < d(\textbf{x}_j, \textbf{x}_i)$. If two examples form a TL then either one is noise or it is a borderline case \cite{Batista:2004:SBS:1007730.1007735}. This information can then be used to under-sample the majority class \cite{Batista:2004:SBS:1007730.1007735}.
	
	One-sided selection (OSS) \cite{Kubat97addressingthe} applies TL to remove borderline and noisy majority class examples then applies CNN to remove majority examples far from the decision border.
	
	Neighbourhood cleaning rule (NCL) \cite{Laurikkala:2001:IID:648155.757340} uses the edited nearest neighbour rule (ENN) to remove majority class examples. ENN removes examples whose label differs from the class of at least two of its nearest 3 neighbours. NCL examines each example $\textbf{x}_i$ and its 3 nearest neighbours. If $\textbf{x}_i$ belongs to the majority class and the neighbours contradict this class then $\textbf{x}_i$ is removed \cite{Batista:2004:SBS:1007730.1007735}. If $\textbf{x}_i$ belongs to the minority class and the neighbours contradict this class then the neighbours from the majority class are removed \cite{Batista:2004:SBS:1007730.1007735}.
	
	Instance threshold hardening (ITH) \cite{Smith:2014:ILA:2843614.2843686} uses the probability estimates from a classifier when k-fold cross validation is applied to only select those examples from the majority class that have the highest probability in order to match the number of examples in the minority class.
	
	NearMiss-1 (NM-1) \cite{mani2003knn} picks majority class examples which have the smallest average distance to the three nearest minority class examples.
	
	Cluster centroids (CC) applies the $k$-means algorithm with $N$ clusters to the majority class and uses the coordinates of the cluster centroids as the majority sample.	
	
	\subsubsection{Combination}
	SMOTE + TL \cite{batista2003balancing} first applies SMOTE then applies TL. Applying SMOTE can cause minority samples to extend too deep into the majority class space or the opposite where majority class samples extend too deep into the minority class space \cite{batista2003balancing}. TL is used as a data cleaning method to remove examples from both classes to produce well-defined class clusters \cite{batista2003balancing}.
	
	SMOTE + ENN works similarly to SMOTE + TL but ENN is more aggressive at removing samples than TL \cite{Batista:2004:SBS:1007730.1007735}.
	
	\subsection{Metrics}
	The number of true positives (TP), true negatives (TN), false positives (FP) and false negatives (FN) are used to define several metrics which are used to compare the results in this paper. The true positive rate (TPR) or sensitivity defines the proportion of actual positives which are predicted as positive. The TPR is calculated as $\frac{TP}{TP + FN}$. The true negative rate (TNR) or specificity defines the proportion of actual negatives which are predicted as negative. The TNR is calculated as $\frac{TN}{TN + FP}$. 
	
	Accuracy is typically a poor measure of quality for imbalanced datasets as classifiers tend to bias towards the majority class \cite{Batista:2004:SBS:1007730.1007735, Chawla:2004:ESI:1007730.1007733}. Several balanced performance metrics are used instead. The balanced accuracy (BACC) provides an equal weighting in TPR and TNR and is calculated as $\frac{TPR + TNR}{2}$. Matthews correlation coefficient (MCC) also provides a balanced measure of classification quality and is scored between -1 and 1. The MCC is calculated as: $\frac{(TP \times TN) - (FP \times FN)}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$. Informedness is similar to balanced accuracy but ranges between -1 and 1 and is defined as $TPR + TNR - 1$.
	
	
	\section{Methods}
	\subsection{Approach}
	We aimed to evaluate a set of statistical and machine learning classification techniques on treatment default data sets and compare it to that applied to other related classification datasets. A variety of classification techniques were chosen to test against: ANN, SVM using a linear, RBF and polynomial kernel, logistic regression, decision tree, AdaBoost, random forest, $k$NN, Gaussian naive Bayes, Bernoulli naive Bayes, ELM and CLC. These were chosen to include a selection of well known techniques, ensemble techniques and newer techniques that have shown promising results in other studies. 
	
	Many datasets have substantially fewer defaulters than those cured. However, most classification algorithms expect a 50:50 split between classes. We therefore include an analysis of several data balancing algorithms: CC, ENN, IHS, NM-1, NCR, OSS, RUS, TL, ADASYN, ROS, SMOTE, ensemble of SMOTE + EEN and ensemble of SMOTE + TL. These balancing techniques were chosen to facilitate an evaluation of both over-sampling and under-sampling techniques. Included in the evaluation is techniques that use simple approaches and as others that use sophisticated algorithms.
	
	All the experiments use stratified 5-fold validation to divide a single dataset into multiple training and testing datasets. Stratified $k$-fold divides the dataset into $k$ segments and ensures that each segment contains the same ratio of positive and negative examples as the dataset as a whole. $k$ number of training and testing sets are created by using one segment as the testing dataset and every other segment as the training data. This is repeated such that every segment is used as testing data. Results for each fold are then averaged before being presented.
	
	\subsubsection{Parameter tuning}
	\label{parameter_tuning}
	We want to determine how important parameter tuning is for our datasets and its effect on each classifier. In addition we want to see if the addition of a data balancing algorithm could improve the classification. We first test each classifier at its default parameters. In the case where a classifier does not have default parameters available, we provide our own reasonable defaults and label those classifiers accordingly. To see the effect of a data balancing algorithm on the classifiers with default parameters, we will use the data balancer that results in the highest BARR. Finally, for each classifier we search a grid of reasonable parameters and select the parameters that yield the highest BARR. The grid includes the parameters of the classifier as well as the different data balancing techniques.
	
	Over-fitting the classifiers was a concern when applying the grid search. To prevent the classifiers from over-fitting the parameter grid was kept fairly coarse and three runs of 5-fold cross-validation was executed and the results averaged. To provide fair comparison between parameter sets, the data balancer, stratified $k$-fold algorithm and classifier itself use fixed initialisation values. These initialisation values differ across each of the three runs. This ensures that each parameter set utilises the same training and testing folds per run but that these folds differ across runs. Stratified 5-fold cross validation was used to ensure that the ratio of defaulters to non-defaulters remained consistent in each fold. 5-fold was used instead of 10-fold to ensure that there was a reasonable sample of defaulters per fold.
	
	To test each of the three scenarios, three runs of stratified 5-fold cross validation is executed. Each run uses a unique random initialisation value for the data balancer, stratified $k$-fold algorithm and classifier. This allows each run to use a different initialisation parameter and ensures that the same folds are used for each classifier on each scenario. The initialisation values are saved in the experiment results to facilitate reproducibility. We used BARR to compare the results between classifiers and scenarios.
	
	\subsubsection{Comparison of classification techniques}
	\label{comparision_of_classification_technique}
	This experiment focuses on the comparing each optimised classifier in detail. The same testing procedure is used as outlined in section \ref{parameter_tuning} except in this experiment we record several additional metrics to compare the classification techniques in detail: TPR, TNR, BARR, MCC, informedness and time to fit each training fold. In addition a receiver operating characteristic (ROC) curve is also plotted as an additional means of comparing the classification techniques and to indicate what TPR can be achieved for an acceptable false positive rate (FPR). To determine how the results generalise against the other similar classification datasets, a final scatter plot is presented which compares difference of TPR and FPR from the median of each for every classifier on every dataset.
	
	The recorded metrics will allow us to determine which classifier is best suited for our treatment default datasets but also allow us to see if the results generalise to the credit default datasets and to reason over why we see those results.
	
	\subsubsection{Comparison of data balancing algorithms}
	The second experiment compares each data balancing technique on each classifier. The optimal parameters for each classifier are used for every data balancer. As in experiment 1, the highest BCR is used to determine the optimal parameters. Each classifier is run using every data balance technique and the metrics outlined in section \ref{method-experiment-1} is recorded. To ensure repeatable results, 10 runs of 5-fold cross validation is executed and the results averaged. A set of unique randomly selected initialisation keys are selected for each run. These are used so that k-fold cross validation, data balancing and classification algorithms produce deterministic output. The initialisation values are saved in the results to ensure they can be reproduced.
	
	A bar chart is plotted with each classifier against the MCC for the treatment default datasets. As in section \ref{method-experiment-1}, a scatter plot is presented which compares difference of true positive and false positive from the median of each for each classifier on each data balancing algorithm on each dataset. 
	
	\subsubsection{Comparison of feature selection algorithms}
	The third experiment investigates a number of different feature selection approaches in order to improve overall classification through the removal of features that each algorithm deems irrelevant to the classification. A number of feature selection strategies were selected for this experiment: Analysis of variance (ANOVA) F-value, logistic regression, linear SVM, Bernoulli naive Bayes, decision tree, random forest. For the ANOVA F-test, a p-value of greater 0.05 was selected to 
	\subsubsection{An analysis of time to default in the Lima Peru dataset}
	
	\subsection{Evaluation}
	
	\subsubsection{Data sets}
	\begin{table*}
		\centering
		\caption{Data set summary}
		\label{table:australian_dataset}
		\begin{tabular}{c|c|c|c|c} \hline	
			Data set&Entries&Number of numerical features&Number of categorical features&Data balance ratio (Cured:Default)\\ \hline
			Lima Peru TB \cite{Lackey:10356751520150601}&1186&0&15&8.65:1 \\
			German Credit Scoring&1000&7&13&2.33:1 \\
			Australian Credit Scoring&690&6&8&1.25:1
		\end{tabular}
	\end{table*}
	
	\section{Implementation}
	
	\section{Results}
	\begin{enumerate}
		\item Tables with true positive, true negative, false positive and false negative rate for each classifier on each dataset
		\item ROC curves which can be used to determine true positive for an acceptable amount of false positives for each classification technique on each dataset
		\item Graphs which outline difference in accuracy of each data balancing technique for each dataset
		\item Discussion on best classification technique
		\item Discussion on data balancing results
		\item Outline similarities and differences in results between the two TB datasets and between TB and financial datasets and determine if they are similar enough that results for the German and Australian credit scoring datasets could be used for the two TB datasets.
	\end{enumerate}
	
	\section{Conclusions and Future Work}
	\begin{enumerate}
		\item Likely something along the lines of utilising more temporal based data so that classification is not just done at registration but also at each check-up for example. Future work may also be the testing of more classification techniques as well as datasets from other parts of the world.
	\end{enumerate}
	
	%
	% The following two commands are all you need in the
	% initial runs of your .tex file to
	% produce the bibliography for the citations in your paper.
	\bibliographystyle{abbrv}
	\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
	% You must have a proper ".bib" file
	%  and remember to run:
	% latex bibtex latex latex
	% to resolve all references
	%
	%\balancecolumns
\end{document}
